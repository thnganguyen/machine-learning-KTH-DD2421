{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import monkdata as m\n",
    "import dtree as d\n",
    "from drawtree_qt5 import drawTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(datasets_names, datasets):\n",
    "    for dataset_name, dataset in zip(datasets_names, datasets):\n",
    "        print(dataset_name, ':' , round(d.entropy(dataset),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_matrix(datasets, attribute_index, max_att_list):\n",
    "    entropy_matrix = np.zeros((len(datasets), len(m.attributes[attribute_index].values)))\n",
    "    for idx, dataset in enumerate(datasets):\n",
    "        att = m.attributes[max_att_list[idx]]\n",
    "        for j, v in enumerate(att.values):\n",
    "            entropy_matrix[idx,j] = d.entropy(d.select(dataset, att, v))\n",
    "    print(entropy_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(datasets):\n",
    "    attributes_names = ['A1', 'A2', 'A3', 'A4', 'A5', 'A6']\n",
    "    information_gain_matrix = np.zeros((len(datasets), len(m.attributes)))\n",
    "    for idx, dataset in enumerate(datasets):\n",
    "        for i in range(len(attributes_names)):\n",
    "            average_gain = round(d.averageGain(dataset, m.attributes[i]),4)\n",
    "            information_gain_matrix[idx, i] = average_gain\n",
    "    return information_gain_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_information_gain(datasets):\n",
    "    attributes_names = ['A1', 'A2', 'A3', 'A4', 'A5', 'A6']\n",
    "    information_gain_matrix = information_gain(datasets)\n",
    "    maximum_information_gain_matrix = np.zeros((len(datasets),2),dtype=object)\n",
    "    max_att_list = []\n",
    "    for i in range(len(datasets)):\n",
    "        inf_gain_maximum = max(information_gain_matrix[i])\n",
    "        if inf_gain_maximum == 0:\n",
    "            e = 0,0\n",
    "            max_att_list.append(0)\n",
    "        if inf_gain_maximum != 0:\n",
    "            x, = np.where(information_gain_matrix[i] == inf_gain_maximum)\n",
    "            e = attributes_names[int(x)], inf_gain_maximum\n",
    "            maximum_information_gain_matrix[i] = e\n",
    "            max_att_list.append(int(x))\n",
    "        #flat_list = [item for sublist in max_att_list for item in sublist]\n",
    "    return maximum_information_gain_matrix, max_att_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tree_by_attribute_and_value(dataset, attribute_idx):\n",
    "    attribute_values = m.attributes[attribute_idx].values\n",
    "    attribute_values_list = [[i] for i in list(attribute_values)]\n",
    "    dataset_by_attribute_and_value = []\n",
    "    for value in attribute_values:\n",
    "        dataset_by_attribute_and_value.append(d.select(dataset, m.attributes[attribute_idx], value))\n",
    "    return dataset_by_attribute_and_value, attribute_values_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_buildTree(datasets):\n",
    "    datasets_trees = []\n",
    "    for dataset in datasets:\n",
    "        datasets_trees.append(d.buildTree(dataset, m.attributes))\n",
    "    return datasets_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_correct_incorrect_classification(datasets, test_datasets, datasets_names):\n",
    "    datasets_trees = perform_buildTree(datasets)\n",
    "    check = {}\n",
    "    check_e = np.zeros((len(datasets), 2))\n",
    "    for i, dataset, dataset_name, dataset_tree, test_dataset in zip(range(len(datasets)),datasets, datasets_names, datasets_trees, test_datasets):   \n",
    "        correct_classification = round(d.check(dataset_tree, test_dataset),3)\n",
    "        check[dataset_name] = correct_classification\n",
    "        err = round(1 - d.check(dataset_tree, dataset),3), round((1 - correct_classification),3)\n",
    "        check_e[i] = err\n",
    "    return check, check_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(data, fraction):\n",
    "    ldata = list(data)\n",
    "    random.shuffle(ldata)\n",
    "    breakPoint = int(len(ldata) * fraction)\n",
    "    return ldata[:breakPoint], ldata[breakPoint:]\n",
    "\n",
    "#monk1train, monk1val = partition(m.monk1, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_names = ['MONK-1', 'MONK-2', 'MONK-3']\n",
    "test_datasets_names = ['monk1test', 'monk2test', 'monk3test']\n",
    "datasets = [m.monk1, m.monk2, m.monk3]\n",
    "test_datasets = [m.monk1test, m.monk2test, m.monk3test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONK-1 : 1.0\n",
      "MONK-2 : 0.957\n",
      "MONK-3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "compute_entropy(datasets_names, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monk1test : 1.0\n",
      "monk2test : 0.914\n",
      "monk3test : 0.998\n"
     ]
    }
   ],
   "source": [
    "compute_entropy(test_datasets_names, test_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Information gain of the three main data sets by attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0753 0.0058 0.0047 0.0263 0.287  0.0008]\n",
      " [0.0038 0.0025 0.0011 0.0157 0.0173 0.0062]\n",
      " [0.0071 0.2937 0.0008 0.0029 0.2559 0.0071]]\n"
     ]
    }
   ],
   "source": [
    "#pd.DataFrame(information_gain(datasets), index=datasets_names, columns=attributes_names)\n",
    "print(information_gain(datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Maximum values by attribute per dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_information_gain, max_att = maximum_information_gain(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['MONK-1' 'A5' 0.287]\n",
      " ['MONK-2' 'A5' 0.0173]\n",
      " ['MONK-3' 'A2' 0.2937]]\n"
     ]
    }
   ],
   "source": [
    "print(np.hstack(([[i] for i in list(datasets_names)], maximum_information_gain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.93831535 0.94807824 0.90817835]\n",
      " [0.91034806 1.         0.96333555 0.877962  ]\n",
      " [0.91829583 0.8296071  0.37764632 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "entropy_matrix(datasets, 4, max_att)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 5: First split:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Data set monk1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset `monk1` according to the `maximum_information_gain` per `attribute_value`. Attribute `A5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_monk1, monk1_values = split_tree_by_attribute_and_value(m.monk1, 4)\n",
    "split_monk2, monk2_values = split_tree_by_attribute_and_value(m.monk2, 4)\n",
    "split_monk3, monk3_values = split_tree_by_attribute_and_value(m.monk3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_information_gain_monk1_a5, max_att_monk1_a5 = maximum_information_gain(split_monk1)\n",
    "maximum_information_gain_monk2_a5, max_att_monk2_a5 = maximum_information_gain(split_monk2)\n",
    "maximum_information_gain_monk3_a2, max_att_monk3_a2 = maximum_information_gain(split_monk3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.     0.     0.     0.     0.     0.    ]\n",
      " [0.0402 0.0151 0.0373 0.0489 0.     0.0258]\n",
      " [0.0331 0.0022 0.018  0.0191 0.     0.0451]\n",
      " [0.2063 0.0339 0.0259 0.0759 0.     0.0033]]\n",
      "   \n",
      "[[1 0 0]\n",
      " [2 'A4' 0.0489]\n",
      " [3 'A6' 0.0451]\n",
      " [4 'A1' 0.2063]]\n"
     ]
    }
   ],
   "source": [
    "print(information_gain(split_monk1))\n",
    "print('   ')\n",
    "print(np.hstack((monk1_values, maximum_information_gain_monk1_a5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0457 0.0785 0.1802 0.1401 0.     0.0048]\n",
      " [0.0026 0.0325 0.0457 0.0258 0.     0.0073]\n",
      " [0.0009 0.0102 0.0333 0.002  0.     0.0043]\n",
      " [0.0022 0.0496 0.0158 0.003  0.     0.0043]]\n",
      "   \n",
      "[[1 'A3' 0.1802]\n",
      " [2 'A3' 0.0457]\n",
      " [3 'A3' 0.0333]\n",
      " [4 'A2' 0.0496]]\n"
     ]
    }
   ],
   "source": [
    "print(information_gain(split_monk2))\n",
    "print('   ')\n",
    "print(np.hstack((monk2_values, maximum_information_gain_monk2_a5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0015 -0.      0.001   0.0501  0.8183  0.001 ]\n",
      " [ 0.0507  0.      0.0345  0.021   0.4767  0.0265]\n",
      " [ 0.0488  0.      0.086   0.1217  0.0804  0.004 ]]\n",
      "   \n",
      "[[1 'A5' 0.8183]\n",
      " [2 'A5' 0.4767]\n",
      " [3 'A4' 0.1217]]\n"
     ]
    }
   ],
   "source": [
    "print(information_gain(split_monk3))\n",
    "print('   ')\n",
    "print(np.hstack((monk3_values, maximum_information_gain_monk3_a2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 5, 0]\n",
      "[2, 2, 2, 1]\n",
      "[4, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "print(max_att_monk1_a5)\n",
    "print(max_att_monk2_a5)\n",
    "print(max_att_monk3_a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Decision Trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_class, incorrect_class = check_correct_incorrect_classification(datasets, test_datasets, datasets_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MONK-1': 0.829, 'MONK-2': 0.692, 'MONK-3': 0.944}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['MONK-1' '0.0' '0.171']\n",
      " ['MONK-2' '0.0' '0.308']\n",
      " ['MONK-3' '0.0' '0.056']]\n"
     ]
    }
   ],
   "source": [
    "print(np.hstack(([[i] for i in list(datasets_names)], incorrect_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
