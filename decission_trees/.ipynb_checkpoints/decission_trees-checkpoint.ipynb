{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import monkdata as m\n",
    "import dtree as d\n",
    "from drawtree_qt5 import drawTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(datasets_names, datasets):\n",
    "    for dataset_name, dataset in zip(datasets_names, datasets):\n",
    "        print(dataset_name, ':' , round(d.entropy(dataset),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_information_gain(datasets):\n",
    "    attributes_names = ['A1', 'A2', 'A3', 'A4', 'A5', 'A6']\n",
    "    information_gain_matrix = information_gain(datasets)\n",
    "    maximum_information_gain_matrix = np.zeros((len(datasets),2),dtype=object)\n",
    "    for i in range(len(datasets)):\n",
    "        inf_gain_maximum = max(information_gain_matrix[i])\n",
    "        if inf_gain_maximum == 0:\n",
    "            e = 'NA',0\n",
    "        if inf_gain_maximum != 0:\n",
    "            x, = np.where(information_gain_matrix[i] == inf_gain_maximum)\n",
    "            e = attributes_names[int(x)], inf_gain_maximum\n",
    "        maximum_information_gain_matrix[i] = e\n",
    "    return maximum_information_gain_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(datasets):\n",
    "    attributes_names = ['A1', 'A2', 'A3', 'A4', 'A5', 'A6']\n",
    "    information_gain_matrix = np.zeros((len(datasets), len(m.attributes)))\n",
    "    for idx, dataset in enumerate(datasets):\n",
    "        for i in range(len(attributes_names)):\n",
    "            average_gain = round(d.averageGain(dataset, m.attributes[i]),4)\n",
    "            information_gain_matrix[idx, i] = average_gain\n",
    "    return information_gain_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tree_by_attribute_and_value(dataset, attribute_idx):\n",
    "    attribute_values = m.attributes[attribute_idx].values\n",
    "    attribute_values_list = [[i] for i in list(attribute_values)]\n",
    "    dataset_by_attribute_and_value = []\n",
    "    for value in attribute_values:\n",
    "        dataset_by_attribute_and_value.append(d.select(dataset, m.attributes[attribute_idx], value))\n",
    "    return dataset_by_attribute_and_value, attribute_values_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_buildTree(datasets):\n",
    "    datasets_trees = []\n",
    "    for dataset in datasets:\n",
    "        datasets_trees.append(d.buildTree(dataset, m.attributes))\n",
    "    return datasets_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_correct_incorrect_classification(datasets, test_datasets, datasets_names):\n",
    "    datasets_trees = perform_buildTree(datasets)\n",
    "    check = {}\n",
    "    check_e = np.zeros((len(datasets), 2))\n",
    "    for dataset, dataset_name, dataset_tree, test_dataset in zip(datasets, datasets_names, datasets_trees, test_datasets):   \n",
    "        correct_classification = round(d.check(dataset_tree, test_dataset),3)\n",
    "        check[dataset_name] = correct_classification\n",
    "       \n",
    "        check_e[0] = round(1 - d.check(dataset_tree, dataset),3)\n",
    "        check_e[1] = round((1 - correct_classification),3)\n",
    "    return check, check_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def partition(data, fraction):\n",
    "    ldata = list(data)\n",
    "    random.shuffle(ldata)\n",
    "    breakPoint = int(len(ldata) * fraction)\n",
    "    return ldata[:breakPoint], ldata[breakPoint:]\n",
    "\n",
    "#monk1train, monk1val = partition(m.monk1, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_names = ['monk1', 'monk2', 'monk3']\n",
    "test_datasets_names = ['monk1test', 'monk2test', 'monk3test']\n",
    "datasets = [m.monk1, m.monk2, m.monk3]\n",
    "test_datasets = [m.monk1test, m.monk2test, m.monk3test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monk1 : 1.0\n",
      "monk2 : 0.957\n",
      "monk3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "compute_entropy(datasets_names, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monk1test : 1.0\n",
      "monk2test : 0.914\n",
      "monk3test : 0.998\n"
     ]
    }
   ],
   "source": [
    "compute_entropy(test_datasets_names, test_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Information gain of the three main data sets by attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0753 0.0058 0.0047 0.0263 0.287  0.0008]\n",
      " [0.0038 0.0025 0.0011 0.0157 0.0173 0.0062]\n",
      " [0.0071 0.2937 0.0008 0.0029 0.2559 0.0071]]\n"
     ]
    }
   ],
   "source": [
    "#pd.DataFrame(information_gain(datasets), index=datasets_names, columns=attributes_names)\n",
    "print(information_gain(datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Maximum values by attribute per dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['monk1' 'A5' 0.287]\n",
      " ['monk2' 'A5' 0.0173]\n",
      " ['monk3' 'A2' 0.2937]]\n"
     ]
    }
   ],
   "source": [
    "print(np.hstack(([[i] for i in list(datasets_names)], maximum_information_gain(datasets))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 5: First split:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Data set monk1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset `monk1` according to the `maximum_information_gain` per `attribute_value`. Attribute `A5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_monk1, monk1_values = split_tree_by_attribute_and_value(m.monk1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.     0.     0.     0.     0.     0.    ]\n",
      " [0.0402 0.0151 0.0373 0.0489 0.     0.0258]\n",
      " [0.0331 0.0022 0.018  0.0191 0.     0.0451]\n",
      " [0.2063 0.0339 0.0259 0.0759 0.     0.0033]]\n"
     ]
    }
   ],
   "source": [
    "print(information_gain(split_monk1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 'NA' 0]\n",
      " [2 'A4' 0.0489]\n",
      " [3 'A6' 0.0451]\n",
      " [4 'A1' 0.2063]]\n"
     ]
    }
   ],
   "source": [
    "print(np.hstack((monk1_values, maximum_information_gain(split_monk1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Decision Trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_class, incorrect_class = check_correct_incorrect_classification(datasets, test_datasets, datasets_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'monk1': 0.829, 'monk2': 0.692, 'monk3': 0.944}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.   ],\n",
       "       [0.056, 0.056],\n",
       "       [0.   , 0.   ]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
